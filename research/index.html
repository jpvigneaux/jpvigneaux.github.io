<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>research | Juan Pablo Vigneaux</title> <meta name="author" content="Juan Pablo Vigneaux"> <meta name="description" content="Interests and projects"> <meta name="keywords" content="information, entropy, topology, measure-theory, category-theory, magnitude, optimal-transport,"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jpvigneaux.github.io/research/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Juan Pablo </span> Vigneaux</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">courses</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">research</h1> <p class="post-description">Interests and projects</p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="ai-interpretability">AI interpretability</h1> <p>I am currently working on AI interpretability, with a particular focus on transformer-based large language models (LLMs). I am actively working on two projects:</p> <h3 id="geometry-of-llms-latent-representations-and-emergence-of-compositional-features">Geometry of LLMs’ latent representations and emergence of compositional features.</h3> <p>Previous work point to a nontrivial intrinsic geometry of token embeddings: they can be seen as points of a stratified space <a class="citation" href="#Robinson2024">(Robinson et al., 2024)</a>, and different models (even trained on different modalities) have comparable embeddings <a class="citation" href="#Roads2020">(Roads &amp; Love, 2020; Luo et al., 2024)</a>. I have computed that in language models such as GPT-2, k-means clustering or hierarchical clustering of the token embeddings reveal some degree of organization that combines semantics and potential syntactic roles. As a first approximation, a <em>primitive feature</em> is a characteristic of an embedded token (possibly shared by members of cluster) that is exploited by some attention or MLP block. Instead of taking the matrices of these blocks as the main objects of interest, my focus is on the coordinate-independent geometric entities (e.g. linear subspaces) that the matrices define in residual space, e.g. via their kernels, images, and singular vectors. Rather than interpreting the singular vectors directly in terms of input tokens, as in <a class="citation" href="#Elhage2021">(Elhage et al., 2021)</a> or <a class="citation" href="#Dar2023">(Dar et al., 2023)</a>, I interpret them as providing incremental modifications to the initial embeddings, “constructing” new context-dependent features from the primitives (thus obtaining <em>compositional features</em>); the interaction with layer norm might contribute to the emergence of discrete classes <a class="citation" href="#Winsor2022">(Winsor, 2022)</a>. Since deeper layers do not only read the initial token embeddings but also the modifications introduced by previous blocks, we need new tools to quantify the relevance of modifications introduced at each layer. A combination of hierarchical clustering and matrix-based input-independent analysis of heads will give us a hierarchy of coarse grained descriptions of the action of each block on tokens and a coarse grained view of the interaction between different blocks.</p> <h3 id="understanding-internal-representations-of-syntax-in-llms-via-mechanistic-interpretability">Understanding internal representations of syntax in LLMs via mechanistic interpretability</h3> <p><em>Joint project with Aman Burman (undergraduate student) and Matilde Marcolli.</em> Transformer-based language models are able to produce text that follows syntactic rules; such ability suddenly emerges in a brief period during training <a class="citation" href="#Chen2024">(Chen et al., 2024)</a>. By analogy with some toy models (see e.g. <a class="citation" href="#Li2023">(Li et al., 2023)</a>), we can hypothesize that a transformer encodes syntax as part of an internal “world representation”. We are using tools from mechanistic interpretability on small language models to extract encoded syntactic features, understand how they are distributed across layers, and identify which subnetworks or <em>circuits</em> utilize this information. We are investigating whether the syntactic processing in language models is hierarchical and analogous to syntactic trees in linguistics. Here we have in mind, in particular, the mathematical formalization of Chomsky’s minimalist program in the language of Hopf algebras <a class="citation" href="#Marcolli2023">(Marcolli et al., 2023; Marcolli et al., 2023)</a>. We want to refine the analysis in <a class="citation" href="#Manning2020">(Manning et al., 2020)</a>, which shows that the activations of attention heads are correlated with syntactic binary relations, but did not explore the mechanisms by which these relations are assembled into more complex trees.</p> <h1 id="mathematics-of-information">Mathematics of information</h1> <p>More broadly, I’m interested in mathematical aspects of <strong>information theory</strong>, particularly in connection with category theory and geometry (metric geometry, geometric measure theory, …). My work in this regard can be organized around three axes:</p> <ul> <li><a href="#minfo-topology">Topological characterization of information measures</a></li> <li><a href="#dimension">Information dimension and measures with geometric structure</a></li> <li><a href="#magnitude">Magnitude and diversity</a></li> </ul> <p>Each of these is described in more detail below. I’ve included links to videos and slides of relevant presentations, along other resources.</p> <h3 id="minfo-topology">Topological characterization of information measures</h3> <p>In “simple” terms, <em>information topology</em> regards a statistical system as a generalized topological space (a <em>topos</em>) and identifies Shannon entropy, along other important “measures of information” used in information theory, as an <a href="https://en.wikipedia.org/wiki/Invariant_(mathematics)" rel="external nofollow noopener" target="_blank">invariant</a> associated to this space.</p> <p><a href="https://en.wikipedia.org/wiki/Topos" rel="external nofollow noopener" target="_blank">Toposes or topoi</a> are an abstraction of topological spaces in the language of category theory and sheaves introduced by Grothendieck and his collaborators (Artin, Verdier,…). Toposes allow richer cohomology theories than set-theoretic topological spaces, and some of these theories (e.g. étale cohomology) play a key role in modern algebraic geometry. Moreover, these <em>Grothendieck toposes</em> are particular cases of <em>elementary toposes</em>, which are “nice” categories with properties analogous to those of the category of sets that play an important role in logic.</p> <p>Baudot and Bennequin <a class="citation" href="#Baudot2015">(Baudot &amp; Bennequin, 2015)</a> first identified Shannon’s discrete entropy as a toposic invariant of certain categories of discrete observables. My Ph.D. thesis <a class="citation" href="#Vigneaux2019-thesis">(Vigneaux, 2019)</a> and a series of articles extended their results in several directions. Namely, the general homological constructions were abstracted from the concrete setting of discrete variables via <em>information structures</em> (categories that encode the relations of refinement between observables), allowing seamless extensions and generalizations to other settings such as continuous vector-valued observables <a class="citation" href="#Vigneaux2020information">(Vigneaux, 2020)</a>.</p> <p>When the information structure encodes discrete observables, the classical information functions—– Shannon entropy, Tsallis $\alpha$-entropy, Kullback-Leibler divergence—–appear as 1-cocycles; the corresponding “coefficients” of the cohomology are probabilistic functionals (i.e. functions of probability laws). There is also a combinatorial version of the theory (coefficients are functions of histograms) where the only 0-cocycle is the exponential function and the 1-cocycles are generalized multinomial coefficients (Fontené-Ward) <a class="citation" href="#Vigneaux2023characterization">(Vigneaux, 2023)</a>. There is an asymptotic relation between the combinatorial and probabilistic cocycles.</p> <p>For information structures that contain continuous vector-valued observables (besides discrete ones), the only new degree-one cocycles are Shannon’s differential entropy entropy and the dimension (of the support of the measure) <a class="citation" href="#Vigneaux-GSI21-cohomology">(Vigneaux, 2021)</a>. This constitutes a novel algebraic characterization of differential entropy.</p> <p>Information cohomology has seen some advances in the last years. Marcolli and Manin <a class="citation" href="#Manin2020homotopy">(Manin &amp; Marcolli, 2020)</a> related information structures with other homotopy- and category-theoretic models of neural information networks. Similar perspectives have been developed more recently by Belfiore and Bennequin <a class="citation" href="#belfiore2021topos">(Belfiore &amp; Bennequin, 2021)</a> to tackle the problem of interpretability of neural networks. They associate to each neural network a certain category equipped with a Grothendieck topology (determined by the connectivity of the neurons), and study the category of sheaves on it, which is a topos. Every topos has an internal logic, and they are linking this internal toposic logic with the classification capabilities that emerge in each layer of a trained neural network (these were previously studied in the experimental article <a class="citation" href="#belfiore2021logical">(Belfiore et al., 2021)</a>).</p> <p><strong>Presentations:</strong></p> <ul> <li> <a href="https://www.youtube.com/watch?v=Nzx9MotxEas" rel="external nofollow noopener" target="_blank">“Cohomological Aspects of Information” [video]</a>, Topos Institute, 2024: I summarize the main results that I have obtained in this domain.</li> <li> <a href="https://www.youtube.com/watch?v=rW76AlxMbrU" rel="external nofollow noopener" target="_blank">“Information Cohomology of Classical Vector-valued Observables” [video]</a>, GSI2021: I provide details on the characterization of the differential entropy and the dimension as the only cohomology classes in degree 1 for systems of vector-valued observables.</li> <li> <a href="https://www.youtube.com/watch?v=uYoW-pGbEWY" rel="external nofollow noopener" target="_blank">“Entropy under disintegrations” [video]</a>, GSI 2021: I explain how every disintegration of a reference measure $\lambda$ induces a chain rule for the generalized differential entropy $S(\rho) = -\int \log (\frac{d\rho}{d\lambda}) d\rho$, which gives a foundation to the extension of information cohomology to more general observables e.g. with values in locally compact topological groups.</li> <li> <a href="https://www.youtube.com/watch?v=eZqo_JcTk3I" rel="external nofollow noopener" target="_blank">“Variations on Information Theory: Categories, Cohomology, Entropy” [video]</a>, IHES, 2016: an older presentation, aimed at probabilistists, where I introduce the notion of (de Rham) cohomology and it’s analogue in our theory.</li> </ul> <p><strong>Other references:</strong></p> <ul> <li> <a href="https://tspace.library.utoronto.ca/bitstream/1807/130552/3/Dub%EF%BF%BD_Hubert_202311_PhD_thesis.pdf" rel="external nofollow noopener" target="_blank">“On the Structure of Information Cohomology”</a>, Ph.D. thesis by Hubert Dubé (U. Toronto), which introduces the Mayer-Vietoris long exact sequence, Shapiro’s lemma and Hochschild-Serre spectral sequence in the framework of information cohomology, and provides some bounds on the cohomological dimension along with new cohomological computations.</li> <li> <a href="https://sites.unimi.it/barbieri/castiglioni.pdf" rel="external nofollow noopener" target="_blank">“Information cohomology and Entropy”</a>, master thesis by Luca Castiglioni (University of Milan).</li> </ul> <h3 id="dimension">Information dimension and measures with geometric structure</h3> <p>From an analytic perspective, the <em>dimension</em> has played an important in information theory since its inception, mainly in connection with quantization. By partitioning $\Rr^d$ into cubes with vertexes in $\mathbb Z^d/n$, one might quantize a continuous probability measure $\rho$ into a measure $\rho_n$ with countable support, whose entropy satisfies \begin{equation}\label{eq:expansion_law} H(\rho_n) = D\ln n + h + o(1), \end{equation} where $D=d$ and $h=h(\rho)$ is the differential entropy of $\rho$ <a class="citation" href="#Kolmogorov1993">(Kolmogorov &amp; Shiryayev, 1993)</a>. Renyi <a class="citation" href="#Renyi1959">(Rényi, 1959)</a> turned this into a definition: if $\rho$ is now a general law and the expansion \eqref{eq:expansion_law} holds for some constants $D,h\in \Rr$, one calls $D$ the information dimension of $\rho$ and $h$ its $d$-dimensional entropy. He wondered about the “topological meaning” of the entropic dimension, which might be noninteger.</p> <p>In <a class="citation" href="#Vigneaux2023typicality">(Vigneaux, 2023)</a> I introduced an <em>asymptotic equipartition property</em> for discrete-continuous mixtures or, more generally, of convex combination of rectifiable measures on $\Rr^d$. In particular, it gives an interpretation for the information dimension $D$ of one of these measures $\rho$: the product $(\Rr^d)^n$ naturally splits into strata of different dimensions, and the typical realizations of $\rho^{\otimes n}$ concentrate on strata of a few dimensions close to $nD$. I also obtained volume estimates (in terms of Hausdorff measures) for the typical realizations in each typical stratum. (A measure $\rho$ is $m$-rectifiable if there exists a set $E$, equal to a countable union of $C^1$ manifolds, such that $\rho$ has a density with respect to the restricted HAusdorff measure $\mathcal H^m|_E$, which is the natural notion of $m$-dimensional volume on $E$.)</p> <p><strong>Presentations:</strong></p> <ul> <li> <a href="/assets/pdf/slides/GSI23-stratified.pdf">“On the entropy of rectifiable and stratified measures” [slides]</a>, GSI 2023, Saint Malò, France.</li> <li> <a href="/assets/pdf/slides/ETH-2023-stratified.pdf">“Typicality for stratified measures” [slides]</a>, ETH Zurich, 2023.</li> </ul> <h3 id="magnitude">Magnitude and diversity</h3> <p>Magnitude <a class="citation" href="#Leinster2008">(Leinster, 2008)</a> is a common categorical generalization of cardinality and of the Euler characteristic of a simplicial complex. It applies to enriched categories, of which metric spaces are a notable example, and in that case gives a new isometric invariant of metric spaces <a class="citation" href="#Leinster2013">(Leinster, 2013)</a>. Applied to infinite metric spaces, this metric invariant—somehow surprisingly—encodes a lot of nontrivial geometric information, such as Minkowski dimension, volume, surface area, etc.<a class="citation" href="#Meckes2015">(Meckes, 2015; Barceló &amp; Carbery, 2018; Gimperlein &amp; Goffeng, 2021)</a>. Partial differential equations, pseudodifferential operators and potential theory have played an important role in establishing these results.</p> <p>In joint work with Stephanie Chen <a class="citation" href="#Chen2023formula">(Chen &amp; Vigneaux, 2023)</a> (SURF program 2022), we gave a new formula for the magnitude of a finite category $\cat{A}$ in terms of the pseudoinverse of the matrix \begin{equation} \zeta:\Ob\cat{A}\times \Ob \cat{A}\to \Zz, \, (a,b)\mapsto |\Hom(a,b)|. \end{equation} This was closer to the definition for posets <a class="citation" href="#Rota1964">(Rota, 1964)</a> that had inspired Leinster. Our work also rederived algebraic properties of the magnitude from properties of the pseudoinverse.</p> <p>In <a class="citation" href="#Vigneaux2024combinatorial">(Vigneaux, 2024)</a> I propose a novel combinatorial interpretation for the inverse or pseudoinverse of $\zeta$, along the lines of <a class="citation" href="#Brualdi2008">(Brualdi &amp; Cvetkovic, 2008)</a>. The interpretation generalizes a celebrated theorem by Philip Hall <a class="citation" href="#Rota1964">(Rota, 1964)</a>: \begin{equation} \zeta^{-1}(a,b)=\sum_{k\geq 0} (-1)^k \# \{ \text{nondegenerate paths between }a\text{ and }b \} \end{equation} when $a$ and $b$ are elements of a finite poset (in this case $\zeta$ is invertible; its inverse is known as Möbius function).</p> <p>What does this have to do with information? Following Boltzmann ideas, entropy can be seen as an extension of cardinality: when all elements of a finite set $X$ are equiprobable, the entropy is $\ln |X|$. In turn, magnitude is a generalization of cardinality, and it is natural to introduce a probabilistic extension of it: “categorical entropy”. Stephanie and I <a class="citation" href="#Chen2023formula">(Chen &amp; Vigneaux, 2023)</a> proposed that categorical entropy is defined on finite categories equipped with a probability $p$ on objects and a “kernel” $\theta:\Ob\cat{A} \times \Ob \cat{A} \to [0,\infty)$ such that $\theta(a,a’)=0$ whenever $a\not\to a’$ via the formula \begin{equation}\label{eq:cat_entropy} \mathcal H(A,p,\theta) = - \sum_{a\in \Ob \cat A} p(a) \ln \left(\sum_{b\in \Ob \cat A} \theta(a,b)p(b) \right). \end{equation} This function shares many “nice” properties with Shannon entropy. In the context of metric spaces equipped with probability, \eqref{eq:cat_entropy} appears as a measure of diversity between species when $p$ is its relative abundance and $\theta$ measures their similarity <a class="citation" href="#Leinster2012">(Leinster &amp; Cobbold, 2012)</a>.</p> <p><strong>Presentations:</strong></p> <ul> <li> <a href="https://www.youtube.com/watch?v=CGzOVeQLLnA" rel="external nofollow noopener" target="_blank">“A Combinatorial Approach to Categorical Möbius Inversion and Magnitude” [video]</a>, Applied Algebraic Topology Network, 2024.</li> <li> <a href="/assets/pdf/slides/GSI23-magnitude.pdf">“Categorical Magnitude and Entropy” [slides]</a>, GSI 2023, Saint Malo, France.</li> </ul> <h1 id="bibliography">Bibliography</h1> <ol class="bibliography"> <li><span id="Robinson2024">Robinson, M., Dey, S., &amp; Sweet, S. (2024). <i>The Structure of the Token Space for Large Language Models</i> (Number arXiv:2410.08993). arXiv.</span></li> <li><span id="Roads2020">Roads, B. D., &amp; Love, B. C. (2020). Learning as the Unsupervised Alignment of Conceptual Systems. <i>Nature Machine Intelligence</i>, <i>2</i>(1), 76–82. https://doi.org/10.1038/s42256-019-0132-2</span></li> <li><span id="Luo2024a">Luo, K., Zhang, B., Xiao, Y., &amp; Lake, B. M. (2024). Finding Unsupervised Alignment of Conceptual Systems in Image-Word Representations. <i>Proceedings of the Annual Meeting of the Cognitive Science Society</i>, <i>46</i>(0).</span></li> <li><span id="Elhage2021">Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., … Olah, C. (2021). A Mathematical Framework for Transformer Circuits. <i>Transformer Circuits Thread</i>.</span></li> <li><span id="Dar2023">Dar, G., Geva, M., Gupta, A., &amp; Berant, J. (2023). Analyzing Transformers in Embedding Space. In A. Rogers, J. Boyd-Graber, &amp; N. Okazaki (Eds.), <i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i> (pp. 16124–16170). Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.acl-long.893</span></li> <li><span id="Winsor2022">Winsor, E. (2022). <i>Re-Examining LayerNorm</i>.</span></li> <li><span id="Chen2024">Chen, A., Shwartz-Ziv, R., Cho, K., Leavitt, M. L., &amp; Saphra, N. (2024). Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in Mlms. <i>The Twelfth International Conference on Learning Representations</i>.</span></li> <li><span id="Li2023">Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., &amp; Wattenberg, M. (2023). Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. <i>The Eleventh International Conference on Learning Representations</i>.</span></li> <li><span id="Marcolli2023">Marcolli, M., Chomsky, N., &amp; Berwick, R. (2023). <i>Mathematical Structure of Syntactic Merge</i> (Number arXiv:2305.18278). arXiv.</span></li> <li><span id="Marcolli2023a">Marcolli, M., Berwick, R. C., &amp; Chomsky, N. (2023). <i>Syntax-Semantics Interface: An Algebraic Model</i> (Number arXiv:2311.06189). arXiv.</span></li> <li><span id="Manning2020">Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U., &amp; Levy, O. (2020). Emergent Linguistic Structure in Artificial Neural Networks Trained by Self-Supervision. <i>Proceedings of the National Academy of Sciences</i>, <i>117</i>(48), 30046–30054. https://doi.org/10.1073/pnas.1907367117</span></li> <li><span id="Baudot2015">Baudot, P., &amp; Bennequin, D. (2015). The Homological Nature of Entropy. <i>Entropy</i>, <i>17</i>(5), 3253–3318.</span></li> <li><span id="Vigneaux2019-thesis">Vigneaux, J. P. (2019). <i>Topology of Statistical Systems: A Cohomological Approach to Information Theory</i> [PhD thesis]. Université de Paris.</span></li> <li><span id="Vigneaux2020information">Vigneaux, J. P. (2020). Information structures and their cohomology. <i>Theory and Applications of Categories</i>, <i>35</i>(38), 1476–1529.</span></li> <li><span id="Vigneaux2023characterization">Vigneaux, J. P. (2023). A characterization of generalized multinomial coefficients related to the entropic chain rule. <i>Aequationes Mathematicae</i>, <i>97</i>(2), 231–255.</span></li> <li><span id="Vigneaux-GSI21-cohomology">Vigneaux, J. P. (2021). Information cohomology of classical vector-valued observables. In F. "Nielsen &amp; F. Barbaresco (Eds.), <i>GSI 2021: Geometric Science of Information</i> (Vol. 12829, pp. 537–546). Springer.</span></li> <li><span id="Manin2020homotopy">Manin, Y., &amp; Marcolli, M. (2020). Homotopy Theoretic and Categorical Models of Neural Information Networks. <i>ArXiv Preprint ArXiv:2006.15136</i>.</span></li> <li><span id="belfiore2021topos">Belfiore, J.-C., &amp; Bennequin, D. (2021). Topos and stacks of deep neural networks. <i>ArXiv Preprint ArXiv:2106.14587</i>.</span></li> <li><span id="belfiore2021logical">Belfiore, J.-C., Bennequin, D., &amp; Giraud, X. (2021). Logical Information Cells I. <i>ArXiv Preprint ArXiv:2108.04751</i>.</span></li> <li><span id="Kolmogorov1993">Kolmogorov, A. N., &amp; Shiryayev, A. N. (1993). <i>Selected Works of A. N. Kolmogorov. Volume III: Information Theory and the Theory of Algorithms</i>. Kluwer Academic Publishers.</span></li> <li><span id="Renyi1959">Rényi, A. (1959). On the dimension and entropy of probability distributions. <i>Acta Mathematica Academiae Scientiarum Hungarica</i>, <i>10</i>(1), 193–215.</span></li> <li><span id="Vigneaux2023typicality">Vigneaux, J. P. (2023). Typicality for stratified measures. <i>IEEE Transactions on Information Theory</i>, <i>69</i>(11), 6922–6940.</span></li> <li><span id="Leinster2008">Leinster, T. (2008). The Euler Characteristic of a Category. <i>Documenta Mathematica</i>, <i>13</i>, 21–49.</span></li> <li><span id="Leinster2013">Leinster, T. (2013). The magnitude of metric spaces. <i>Documenta Mathematica</i>, <i>18</i>, 857–905.</span></li> <li><span id="Meckes2015">Meckes, M. W. (2015). Magnitude, diversity, capacities, and dimensions of metric spaces. <i>Potential Analysis</i>, <i>42</i>, 549–572.</span></li> <li><span id="Barcelo2018">Barceló, J. A., &amp; Carbery, A. (2018). On the magnitudes of compact sets in Euclidean spaces. <i>American Journal of Mathematics</i>, <i>140</i>(2), 449–494.</span></li> <li><span id="Gimperlein2021">Gimperlein, H., &amp; Goffeng, M. (2021). On the magnitude function of domains in Euclidean space. <i>American Journal of Mathematics</i>, <i>143</i>(3), 939–967.</span></li> <li><span id="Chen2023formula">Chen, S., &amp; Vigneaux, J. P. (2023). A formula for the categorical magnitude in terms of the Moore-Penrose pseudoinverse. <i>Bulletin of the Belgian Mathematical Society - Simon Stevin</i>, <i>30</i>(3), 341–353.</span></li> <li><span id="Rota1964">Rota, G.-C. (1964). On the foundations of combinatorial theory I. Theory of Möbius functions. <i>Probability Theory and Related Fields</i>, <i>2</i>(4), 340–368.</span></li> <li><span id="Vigneaux2024combinatorial">Vigneaux, J. P. (2024). A combinatorial approach to categorical Möbius inversion and pseudoinversion. <i>ArXiv Preprint 2407.14647</i>.</span></li> <li><span id="Brualdi2008">Brualdi, R. A., &amp; Cvetkovic, D. (2008). <i>A Combinatorial Approach to Matrix Theory and Its Applications</i>. CRC Press. https://books.google.com/books?id=pwx6t8QfZU8C</span></li> <li><span id="Leinster2012">Leinster, T., &amp; Cobbold, C. A. (2012). Measuring diversity: the importance of species similarity. <i>Ecology</i>, <i>93</i>(3), 477–489.</span></li> </ol> </div> </article> </div> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Juan Pablo Vigneaux. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: May 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0,macros:{salg:"{\\mathfrak}",Rr:"{\\mathbb{R}}",Cc:"{\\mathbb{C}}",Nn:"{\\mathbb{N}}",Qq:"{\\mathbb{Q}}",Zz:"{\\mathbb{Z}}",Ff:"{\\mathbb{F}}",Ex:"{\\mathbin{\\mathbb{E}}}",Pr:"{\\mathbin{\\mathbb{P}}}",Var:"{\\mathbin{\\mathbf{Var}}}",Tr:["{\\operatorname{Tr}\\left( #1\\right)}",1],Bin:"{\\operatorname{Bin}}",Ber:"{\\operatorname{Ber}}",Hom:"{\\operatorname{Hom}}",Ext:"{\\operatorname{Ext}}",Tor:"{\\operatorname{Tor}}",ker:"{\\operatorname{ker}}",coker:"{\\operatorname{coker}}",im:"{\\operatorname{im}}",coim:"{\\operatorname{coim}}",monic:"{\\rightarrowtail}",epic:"{\\twoheadrightarrow}",tr:"{^{\\rm T}}",rank:"{\\operatorname{rank}}",id:"{\\operatorname{id}}",Ob:"{\\operatorname{Ob}}",Mor:"{\\operatorname{Mor}}",sheaf:["{\\mathcal{#1}}",1],cat:["{\\mathbf{#1}}",1],vol:"{\\operatorname{vol}}"}}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-6B8Q37P9Y6"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-6B8Q37P9Y6");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
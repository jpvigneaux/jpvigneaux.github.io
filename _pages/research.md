---
layout: page
title: Research
permalink: /research/
description: Interests and projects
nav: true
nav_order: 1
#display_categories: [work, fun]
horizontal: false
---

I'm broadly interested in the concept of **information**, a multifaceted concept at the crossroads of physics, computer science, pure mathematics and biology.  

The projects described below reflect my main scientific contributions so far.

## Information topology

Baudot and Bennequin {% cite Baudot2015 %} identified Shannon's discrete entropy as a topological invariant of certain categories statistical observables. My Ph.D. thesis and a series of articles {% cite %} gave a general construction in the framew extended their results in several directions.

More precisely, I defined *information structures* as categories that encode the relations of refinement between
different statistical observables. So every information structure gives rise to a ringed site, and one may define information cohomology using the homological tools developed by Artin, Grothendieck, Verdier and their collaborators in the SGA IV ("Cohomologie des topos"). The cohomology depends on a presheaf (contravariant functor) of coefficients. The cocycles enjoy certain recursive properties e.g. the chain rule of entropy.

When the information structure encodes discrete random variables, the classical information functions—
Shannon entropy, Tsallis α-entropy, Kullback-Leibler divergence—appear as 1-cocycles
for appropriate modules of probabilistic coefficients (functions of probability laws).
When the coefficients are combinatorial (functions of histograms), the only 0-cocycle is the exponential function, and the 1-cocycles are generalized multinomial coefficients (FontenéWard). There is an asymptotic relation between the combinatorial and probabilistic
cocycles. 

I The cohomological computations are restricted to the real valued, gaussian case. When coordinates are fixed,
the 1-cocycles are the differential entropy as well as generalized moments. When
computations are done in a coordinate-free manner, with the so-called grassmannian
categories, we recover as the only degree-one cohomology classes the entropy and the
dimension. This constitutes a novel algebraic characterization of differential entropy.

## Rectifiable and stratified measures





## Magnitude and diversity

Leinster

See cardinality as 


## Combinatorial models for generalized entropies



## Other interests


* Optimal transport in classical and quantum probability; functional inequalities.
* Information processing in the brain; neurogeometry; computational neuroscience.
* Information theoretic limits for disentangled representations in the framework of statistical learning theory.
